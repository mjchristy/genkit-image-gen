import { z } from "genkit";
import {
  evaluatorRef
} from "genkit/evaluator";
import { genkitPlugin } from "genkit/plugin";
import { answerAccuracyScore } from "./metrics/answer_accuracy.js";
import {
  answerRelevancyScore,
  deepEqual,
  faithfulnessScore,
  jsonata,
  maliciousnessScore,
  regexp
} from "./metrics/index.js";
import {
  GenkitMetric,
  isGenkitMetricConfig
} from "./types.js";
const PLUGIN_NAME = "genkitEval";
const genkitEvalRef = (metric) => evaluatorRef({
  name: `${PLUGIN_NAME}/${metric.toLocaleLowerCase()}`,
  configSchema: z.undefined(),
  info: {
    label: `Genkit RAG Evaluator for ${metric}`,
    metrics: [metric]
  }
});
function genkitEval(params) {
  return genkitPlugin(`${PLUGIN_NAME}`, async (ai) => {
    genkitEvaluators(ai, params);
  });
}
var index_default = genkitEval;
function fillScores(dataPoint, score, statusOverrideFn) {
  let status = score.status;
  if (statusOverrideFn) {
    status = statusOverrideFn({ score });
  }
  return { testCaseId: dataPoint.testCaseId, evaluation: { ...score, status } };
}
function genkitEvaluators(ai, params) {
  const { metrics } = params;
  if (metrics.length === 0) {
    throw new Error("No metrics configured in genkitEval plugin");
  }
  return metrics.map((metric) => {
    const {
      type,
      judge,
      judgeConfig,
      embedder,
      embedderOptions,
      statusOverrideFn
    } = resolveConfig(metric, params);
    const evaluator = `${PLUGIN_NAME}/${type.toLocaleLowerCase()}`;
    switch (type) {
      case GenkitMetric.ANSWER_RELEVANCY: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing answer relvancy"
          );
        }
        if (!embedder) {
          throw new Error(
            "Embedder must be specified if computing answer relvancy"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Answer Relevancy",
            definition: "Assesses how pertinent the generated answer is to the given prompt"
          },
          async (datapoint) => {
            const answerRelevancy = await answerRelevancyScore(
              ai,
              judge,
              datapoint,
              embedder,
              judgeConfig,
              embedderOptions
            );
            return fillScores(datapoint, answerRelevancy, statusOverrideFn);
          }
        );
      }
      case GenkitMetric.FAITHFULNESS: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing faithfulness"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Faithfulness",
            definition: "Measures the factual consistency of the generated answer against the given context"
          },
          async (datapoint) => {
            const faithfulness = await faithfulnessScore(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, faithfulness, statusOverrideFn);
          }
        );
      }
      case GenkitMetric.MALICIOUSNESS: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing maliciousness"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Maliciousness",
            definition: "Measures whether the generated output intends to deceive, harm, or exploit"
          },
          async (datapoint) => {
            const maliciousness = await maliciousnessScore(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, maliciousness, statusOverrideFn);
          }
        );
      }
      case GenkitMetric.ANSWER_ACCURACY: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing answer accuracy"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Answer Accuracy",
            definition: "Measures how accurately the generated output matches against the reference output"
          },
          async (datapoint) => {
            const answerAccuracy = await answerAccuracyScore(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, answerAccuracy, statusOverrideFn);
          }
        );
      }
      case GenkitMetric.REGEX: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "RegExp",
            definition: "Tests output against the regexp provided as reference"
          },
          async (datapoint) => {
            return fillScores(datapoint, await regexp(datapoint));
          }
        );
      }
      case GenkitMetric.DEEP_EQUAL: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Deep Equals",
            definition: "Tests equality of output against the provided reference"
          },
          async (datapoint) => {
            return fillScores(
              datapoint,
              await deepEqual(datapoint),
              statusOverrideFn
            );
          }
        );
      }
      case GenkitMetric.JSONATA: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "JSONata",
            definition: "Tests JSONata expression (provided in reference) against output"
          },
          async (datapoint) => {
            return fillScores(
              datapoint,
              await jsonata(datapoint),
              statusOverrideFn
            );
          }
        );
      }
    }
  });
}
function resolveConfig(metric, params) {
  if (isGenkitMetricConfig(metric)) {
    return {
      type: metric.type,
      statusOverrideFn: metric.statusOverrideFn,
      judge: metric.judge ?? params.judge,
      judgeConfig: metric.judgeConfig ?? params.judgeConfig,
      embedder: metric.type === GenkitMetric.ANSWER_RELEVANCY ? metric.embedder : void 0,
      embedderOptions: metric.type === GenkitMetric.ANSWER_RELEVANCY ? metric.embedderOptions : void 0
    };
  }
  return { type: metric, ...params };
}
export {
  GenkitMetric,
  index_default as default,
  genkitEval,
  genkitEvalRef,
  genkitEvaluators
};
//# sourceMappingURL=index.mjs.map