import {
  EvalStatusEnum
} from "genkit/evaluator";
import path from "path";
import { getDirName, loadPromptFile, renderText } from "./helper.js";
async function answerAccuracyScore(ai, judgeLlm, dataPoint, judgeConfig) {
  if (!dataPoint.output) {
    throw new Error("Output was not provided");
  }
  if (!dataPoint.reference) {
    throw new Error("Reference was not provided");
  }
  const input = typeof dataPoint.input === "string" ? dataPoint.input : JSON.stringify(dataPoint.input);
  const output = typeof dataPoint.output === "string" ? dataPoint.output : JSON.stringify(dataPoint.output);
  const reference = typeof dataPoint.reference === "string" ? dataPoint.reference : JSON.stringify(dataPoint.reference);
  const prompt = await loadPromptFile(
    path.resolve(getDirName(), "../../prompts/answer_accuracy.prompt")
  );
  const origResp = await ai.generate({
    model: judgeLlm,
    config: judgeConfig,
    prompt: await renderText(prompt, {
      query: input,
      output,
      reference
    })
  });
  const origScore = Number.parseInt(origResp.text);
  if (Number.isNaN(origScore)) {
    throw new Error("Error generating original response for answer accuracy");
  }
  const invResp = await ai.generate({
    model: judgeLlm,
    config: judgeConfig,
    prompt: await renderText(prompt, {
      query: input,
      output: reference,
      reference: output
    })
  });
  const invScore = Number.parseInt(invResp.text);
  if (Number.isNaN(invScore)) {
    throw new Error("Error generating inverted response for answer accuracy");
  }
  const score = (origScore + invScore) / 8;
  return {
    score,
    status: score >= 0.5 ? EvalStatusEnum.PASS : EvalStatusEnum.FAIL
  };
}
export {
  answerAccuracyScore
};
//# sourceMappingURL=answer_accuracy.mjs.map