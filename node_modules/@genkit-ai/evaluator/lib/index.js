"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);
var index_exports = {};
__export(index_exports, {
  GenkitMetric: () => import_types.GenkitMetric,
  default: () => index_default,
  genkitEval: () => genkitEval,
  genkitEvalRef: () => genkitEvalRef,
  genkitEvaluators: () => genkitEvaluators
});
module.exports = __toCommonJS(index_exports);
var import_genkit = require("genkit");
var import_evaluator = require("genkit/evaluator");
var import_plugin = require("genkit/plugin");
var import_answer_accuracy = require("./metrics/answer_accuracy.js");
var import_metrics = require("./metrics/index.js");
var import_types = require("./types.js");
const PLUGIN_NAME = "genkitEval";
const genkitEvalRef = (metric) => (0, import_evaluator.evaluatorRef)({
  name: `${PLUGIN_NAME}/${metric.toLocaleLowerCase()}`,
  configSchema: import_genkit.z.undefined(),
  info: {
    label: `Genkit RAG Evaluator for ${metric}`,
    metrics: [metric]
  }
});
function genkitEval(params) {
  return (0, import_plugin.genkitPlugin)(`${PLUGIN_NAME}`, async (ai) => {
    genkitEvaluators(ai, params);
  });
}
var index_default = genkitEval;
function fillScores(dataPoint, score, statusOverrideFn) {
  let status = score.status;
  if (statusOverrideFn) {
    status = statusOverrideFn({ score });
  }
  return { testCaseId: dataPoint.testCaseId, evaluation: { ...score, status } };
}
function genkitEvaluators(ai, params) {
  const { metrics } = params;
  if (metrics.length === 0) {
    throw new Error("No metrics configured in genkitEval plugin");
  }
  return metrics.map((metric) => {
    const {
      type,
      judge,
      judgeConfig,
      embedder,
      embedderOptions,
      statusOverrideFn
    } = resolveConfig(metric, params);
    const evaluator = `${PLUGIN_NAME}/${type.toLocaleLowerCase()}`;
    switch (type) {
      case import_types.GenkitMetric.ANSWER_RELEVANCY: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing answer relvancy"
          );
        }
        if (!embedder) {
          throw new Error(
            "Embedder must be specified if computing answer relvancy"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Answer Relevancy",
            definition: "Assesses how pertinent the generated answer is to the given prompt"
          },
          async (datapoint) => {
            const answerRelevancy = await (0, import_metrics.answerRelevancyScore)(
              ai,
              judge,
              datapoint,
              embedder,
              judgeConfig,
              embedderOptions
            );
            return fillScores(datapoint, answerRelevancy, statusOverrideFn);
          }
        );
      }
      case import_types.GenkitMetric.FAITHFULNESS: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing faithfulness"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Faithfulness",
            definition: "Measures the factual consistency of the generated answer against the given context"
          },
          async (datapoint) => {
            const faithfulness = await (0, import_metrics.faithfulnessScore)(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, faithfulness, statusOverrideFn);
          }
        );
      }
      case import_types.GenkitMetric.MALICIOUSNESS: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing maliciousness"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Maliciousness",
            definition: "Measures whether the generated output intends to deceive, harm, or exploit"
          },
          async (datapoint) => {
            const maliciousness = await (0, import_metrics.maliciousnessScore)(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, maliciousness, statusOverrideFn);
          }
        );
      }
      case import_types.GenkitMetric.ANSWER_ACCURACY: {
        if (!judge) {
          throw new Error(
            "Judge llms must be specified if computing answer accuracy"
          );
        }
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Answer Accuracy",
            definition: "Measures how accurately the generated output matches against the reference output"
          },
          async (datapoint) => {
            const answerAccuracy = await (0, import_answer_accuracy.answerAccuracyScore)(
              ai,
              judge,
              datapoint,
              judgeConfig
            );
            return fillScores(datapoint, answerAccuracy, statusOverrideFn);
          }
        );
      }
      case import_types.GenkitMetric.REGEX: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "RegExp",
            definition: "Tests output against the regexp provided as reference"
          },
          async (datapoint) => {
            return fillScores(datapoint, await (0, import_metrics.regexp)(datapoint));
          }
        );
      }
      case import_types.GenkitMetric.DEEP_EQUAL: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "Deep Equals",
            definition: "Tests equality of output against the provided reference"
          },
          async (datapoint) => {
            return fillScores(
              datapoint,
              await (0, import_metrics.deepEqual)(datapoint),
              statusOverrideFn
            );
          }
        );
      }
      case import_types.GenkitMetric.JSONATA: {
        return ai.defineEvaluator(
          {
            name: evaluator,
            displayName: "JSONata",
            definition: "Tests JSONata expression (provided in reference) against output"
          },
          async (datapoint) => {
            return fillScores(
              datapoint,
              await (0, import_metrics.jsonata)(datapoint),
              statusOverrideFn
            );
          }
        );
      }
    }
  });
}
function resolveConfig(metric, params) {
  if ((0, import_types.isGenkitMetricConfig)(metric)) {
    return {
      type: metric.type,
      statusOverrideFn: metric.statusOverrideFn,
      judge: metric.judge ?? params.judge,
      judgeConfig: metric.judgeConfig ?? params.judgeConfig,
      embedder: metric.type === import_types.GenkitMetric.ANSWER_RELEVANCY ? metric.embedder : void 0,
      embedderOptions: metric.type === import_types.GenkitMetric.ANSWER_RELEVANCY ? metric.embedderOptions : void 0
    };
  }
  return { type: metric, ...params };
}
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  GenkitMetric,
  genkitEval,
  genkitEvalRef,
  genkitEvaluators
});
//# sourceMappingURL=index.js.map